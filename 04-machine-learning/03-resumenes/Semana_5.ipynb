{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 1 - Caracterizacion de un crecimiento de arboles.\n",
    "\n",
    "```python\n",
    "import c1applet\n",
    "```\n",
    "No cosigo informacion de la libreria c1applet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_study_norm = norm(in_study_data.iloc[:,0:8], axis=0)\n",
    "in_study_data.iloc[:,0:8]/= in_study_norm\n",
    "out_of_study_data/=  in_study_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select 10 random out-of-study prisoner\n",
    "np.random.seed(0)\n",
    "random_subset=np.random.randint(0,m-1,100)\n",
    "\n",
    "#merge with the \n",
    "all_data=pd.concat([in_study_data,out_of_study_data.iloc[random_subset,:]],sort=True)\n",
    "all_data=all_data.fillna(2)\n",
    "all_data['reoffend']=all_data['reoffend'].astype('int')\n",
    "\n",
    "#Plot sentence_time and ave_severity features of the prisoners\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='sentence_time',y='sentence_served',hue='reoffend',data=all_data,palette='Paired')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código que has compartido utiliza la biblioteca tqdm para mostrar una barra de progreso mientras se ejecuta un bucle for con un gran número de iteraciones.\n",
    "\n",
    "Aquí hay una explicación paso a paso del código:\n",
    "\n",
    "from tqdm.auto import tqdm, trange: Esto importa las clases tqdm y trange de la biblioteca tqdm. tqdm es una herramienta de visualización del progreso, y trange es una función que genera un iterador para el bucle for con una barra de progreso.\n",
    "\n",
    "for j in trange(1000000, desc = \"Iterating\"):: Esto inicia un bucle for que se ejecutará un millón de veces. trange(1000000) crea un iterador que itera desde 0 hasta 999999 y muestra una barra de progreso. desc = \"Iterating\" establece la descripción que se mostrará junto a la barra de progreso.\n",
    "\n",
    "if j%50000 == 0:: Esto verifica si el valor actual de j es divisible por 50000 sin dejar residuo. En otras palabras, verifica si j es un múltiplo de 50000.\n",
    "\n",
    "print(\"Finished \" + str(j) + \" iterations\"): Si j es un múltiplo de 50000, imprime un mensaje que indica cuántas iteraciones se han completado hasta ese punto.\n",
    "\n",
    "En resumen, el código muestra una barra de progreso mientras se ejecuta un bucle for con un millón de iteraciones. Cada vez que se completa un múltiplo de 50000 iteraciones, se imprime un mensaje para indicar cuántas iteraciones se han completado hasta ese punto.\n",
    "\n",
    "Esto puede ser útil para tener una idea visual del progreso de un bucle largo y para obtener información sobre el número de iteraciones completadas en momentos específicos durante la ejecución del bucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 810 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e5a4df1d1047278ad82f9cec3bc9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating:   0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0 iterations\n",
      "Finished 50000 iterations\n",
      "Finished 100000 iterations\n",
      "Finished 150000 iterations\n",
      "Finished 200000 iterations\n",
      "Finished 250000 iterations\n",
      "Finished 300000 iterations\n",
      "Finished 350000 iterations\n",
      "Finished 400000 iterations\n",
      "Finished 450000 iterations\n",
      "Finished 500000 iterations\n",
      "Finished 550000 iterations\n",
      "Finished 600000 iterations\n",
      "Finished 650000 iterations\n",
      "Finished 700000 iterations\n",
      "Finished 750000 iterations\n",
      "Finished 800000 iterations\n",
      "Finished 850000 iterations\n",
      "Finished 900000 iterations\n",
      "Finished 950000 iterations\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "for j in trange(1000000, desc = \"Iterating\"):\n",
    "    if j%50000 == 0:\n",
    "     print(\"Finished \" + str(j) + \" iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada = input(\"Enter some text: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genera tabla de gráficos comparativos, X vs Y, para todas las variables\n",
    "sns.pairplot(df_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "# Clase 10-11 Tablas de contingencia, ANOVA y Chi cuadrado.\n",
    "\n",
    "##### Libreria pingouin\n",
    "\n",
    "La biblioteca `pingouin` en Python es una librería estadística que proporciona herramientas para realizar análisis estadísticos y pruebas de hipótesis. Está diseñada para ser fácil de usar y ofrece funciones específicas para tareas comunes en estadísticas y análisis de datos.\n",
    "\n",
    "Algunas de las características y funcionalidades clave de la biblioteca `pingouin` incluyen:\n",
    "\n",
    "1. **Análisis de varianza (ANOVA):** Permite realizar análisis de varianza unidireccional (one-way) y bidireccional (two-way) para comparar las medias entre varios grupos.\n",
    "\n",
    "2. **Pruebas de correlación:** Proporciona funciones para realizar pruebas de correlación, incluyendo la correlación de Pearson, la correlación de Spearman y la correlación de Kendall.\n",
    "\n",
    "3. **Pruebas t para muestras independientes y pareadas:** Facilita la realización de pruebas t para comparar las medias de dos grupos, ya sea para muestras independientes o pareadas.\n",
    "\n",
    "4. **Regresión lineal:** Ofrece funciones para realizar análisis de regresión lineal simple y múltiple.\n",
    "\n",
    "5. **Pruebas no paramétricas:** Incluye pruebas estadísticas no paramétricas, como la prueba de Mann-Whitney y la prueba de Wilcoxon, que no hacen suposiciones sobre la distribución de los datos.\n",
    "\n",
    "6. **Pruebas de normalidad:** Permite realizar pruebas para evaluar la normalidad de los datos, como la prueba de Shapiro-Wilk.\n",
    "\n",
    "La biblioteca `pingouin` se basa en otras bibliotecas populares de Python, como `scipy` y `pandas`, y está diseñada para ser fácil de integrar con el flujo de trabajo de análisis de datos en Python.\n",
    "\n",
    "Para utilizar `pingouin`, generalmente necesitarás instalarlo utilizando `pip install pingouin` y luego importar las funciones necesarias en tu script o Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tabla de contingencia:\n",
    "\n",
    "Una tabla de contingencia es una herramienta en estadística descriptiva que organiza datos en filas y columnas para mostrar la frecuencia de ocurrencia de diversas combinaciones de variables categóricas. Estas tablas son especialmente útiles cuando se trabaja con variables categóricas y se busca entender la relación o asociación entre ellas.\n",
    "\n",
    "```\n",
    "                      | Teléfono | Computadora | Tableta |\n",
    "----------------------|----------|-------------|---------|\n",
    "Joven                 |   15     |      10     |   5     |\n",
    "Adulto                |   20     |      25     |  15     |\n",
    "Anciano               |   5      |      8      |   3     |\n",
    "\n",
    "```\n",
    "\n",
    "##### Crosstab\n",
    "\n",
    "Genera tablas cruzadas que toma como indice valores categoricos vs otra columna categorica, llenandolo con frecuencias de datos repetidos. En otras palabras, crosstab se usa para generar tablas de contingencia.\n",
    "\n",
    "```py\n",
    "pd.crosstab(index=df[\"Offenses_cat\"], columns=df[\"Location_cat\"], normalize=\"columns\")*100\n",
    "```\n",
    "\n",
    "<img src='normalizacion.jpeg'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de Hipótesis\n",
    "\n",
    "<img src='Experiment_design.jpeg'>\n",
    "\n",
    "La prueba de hipótesis es una técnica estadística que forma parte del campo de diseño experimental, al igual que conceptos como ANOVA, ANCOVA y Chi cuadrado.\n",
    "\n",
    "El propósito fundamental de la prueba de hipótesis es evaluar si la aplicación de una variable de cambio (parámetro) a un objeto de estudio provoca cambios significativos en dicho objeto. La meta es determinar si una hipótesis dada puede ser aceptada o rechazada, utilizando un nivel de tolerancia de error conocido como p-valor. Comúnmente, este valor se selecciona entre un 0.05 (5%) y un 0.01 (1%).\n",
    "\n",
    "Inicialmente, se parte de la suposición de que la hipótesis nula (H) es igual a 0 (cero), lo que implica que no existe relación entre las variables. Esta hipótesis nula se contrasta con la hipótesis alternativa, que puede ser H diferente de 0, H > 0 o H < 0.\n",
    "\n",
    "La técnica ANOVA se utiliza para determinar si existen diferencias significativas entre los datos, mientras que la prueba de Chi cuadrado se emplea para evaluar la independencia o relación entre dos variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Chi cuadrado\n",
    "\n",
    "Se emplea para evaluar la independencia o relacion entre variables. En python existe la libreria scipy, que tiene herramientas estadisticas entre ellas la contingencia de chi cuadrado, que recibe como parametro una tabla de contingencia o sea un **crosstab**.\n",
    "\n",
    "```py\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2_contingency(my_contingency_table)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `chi2_contingency` de la librería `scipy.stats` en Python se utiliza para realizar la prueba de independencia chi-cuadrado en una tabla de contingencia. Esta prueba se utiliza para evaluar si hay una asociación estadísticamente significativa entre dos variables categóricas. La tabla de contingencia es una tabla que muestra la distribución conjunta de las frecuencias de dos o más variables categóricas.\n",
    "\n",
    "La firma de la función es la siguiente:\n",
    "\n",
    "```python\n",
    "scipy.stats.chi2_contingency(observed, correction=True, lambda_=None)\n",
    "```\n",
    "\n",
    "- `observed`: La tabla de contingencia observada, es decir, una matriz 2D de frecuencias observadas.\n",
    "- `correction`: Un parámetro booleano que indica si se debe aplicar la corrección de continuidad de Yates. Por defecto, es `True`.\n",
    "- `lambda_`: Un parámetro que controla la devolución de la estadística de prueba. Si no se proporciona, se utiliza la estadística de prueba chi-cuadrado clásica. Si se proporciona un valor lambda, se utiliza la estadística de prueba generalizada de likelihood-ratio (GLR). Por defecto, es `None`.\n",
    "\n",
    "La función devuelve una tupla que contiene cuatro valores:\n",
    "\n",
    "1. **Estadística de prueba chi-cuadrado**: Este valor mide la discrepancia entre las frecuencias observadas y las frecuencias esperadas bajo la hipótesis nula de independencia. Cuanto mayor sea este valor, mayor será la evidencia en contra de la independencia de las variables.\n",
    "\n",
    "2. **Valor p (p-value)**: Indica la probabilidad de obtener una estadística de prueba tan extrema como la observada, asumiendo que la hipótesis nula es verdadera. Un valor p pequeño sugiere que se rechaza la hipótesis nula de independencia.\n",
    "\n",
    "3. **Grados de libertad**: Indica el número de grados de libertad asociados con la estadística de prueba.\n",
    "\n",
    "4. **Frecuencias esperadas**: Una matriz que contiene las frecuencias esperadas bajo la hipótesis nula de independencia.\n",
    "\n",
    "Ejemplo de uso:\n",
    "\n",
    "```python\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "observed = [[10, 20], [30, 40]]\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(observed)\n",
    "\n",
    "print(\"Estadística de prueba chi-cuadrado:\", chi2_stat)\n",
    "print(\"Valor p:\", p_val)\n",
    "print(\"Grados de libertad:\", dof)\n",
    "print(\"Frecuencias esperadas:\", expected)\n",
    "```\n",
    "\n",
    "En este ejemplo, `observed` es la tabla de contingencia observada. La función devuelve la estadística de prueba chi-cuadrado, el valor p, los grados de libertad y las frecuencias esperadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df, cmap=\"Reds\")\n",
    "# cmap = color map\n",
    "# Grafica un mapa de calor de los datos de df, donde los valores rojos representan valores altos\n",
    "# y los valores negros representan valores bajos\n",
    "# el df debe de ser una tabla de contingencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion T Test de pingouin\n",
    "\n",
    "<img src = 'prueba_T.jpeg'>\n",
    "\n",
    "La función `ttest` de la librería `pingouin` en Python se utiliza para realizar pruebas t (t-tests), que son pruebas estadísticas diseñadas para evaluar si hay diferencias significativas entre las medias de dos grupos de datos. Específicamente, `pingouin.ttest` se utiliza para realizar pruebas t independientes y emparejadas.\n",
    "\n",
    "La función tiene la siguiente firma general:\n",
    "\n",
    "```python\n",
    "pingouin.ttest(x, y=None, paired=False, ...)\n",
    "```\n",
    "\n",
    "- `x`: Primer grupo de datos.\n",
    "- `y`: Segundo grupo de datos (opcional para pruebas t independientes, obligatorio para pruebas t emparejadas).\n",
    "- `paired`: Un parámetro booleano que indica si los datos son emparejados o no. Por defecto, es `False` para pruebas t independientes.\n",
    "\n",
    "La función devuelve un objeto `pandas.DataFrame` con información sobre la prueba t. Algunos de los resultados clave incluyen:\n",
    "\n",
    "- **t**: Valor t de la prueba.\n",
    "- **p-val**: Valor p de la prueba, que indica la probabilidad de obtener un resultado al menos tan extremo como el observado, bajo la hipótesis nula de que las medias son iguales.\n",
    "- **CI95%**: Intervalo de confianza del 95% para la diferencia de medias (solo para pruebas t independientes).\n",
    "- **cohen-d**: Tamaño del efecto (coeficiente de Cohen's d).\n",
    "\n",
    "Ejemplo de uso para una prueba t independiente:\n",
    "\n",
    "```python\n",
    "import pingouin as pg\n",
    "\n",
    "# Datos de dos grupos independientes\n",
    "group1 = [23, 25, 28, 30, 32]\n",
    "group2 = [18, 20, 22, 24, 26]\n",
    "\n",
    "result = pg.ttest(group1, group2)\n",
    "print(result)\n",
    "```\n",
    "\n",
    "Este ejemplo compara las medias de `group1` y `group2` utilizando una prueba t independiente y muestra el resultado en forma de un DataFrame con los valores t, p-val, CI95%, cohen-d, entre otros.\n",
    "\n",
    "Recuerda que para pruebas t emparejadas, necesitarías proporcionar los dos conjuntos de datos y establecer `paired=True`. La función es bastante flexible y ofrece opciones adicionales para manejar diferentes situaciones, por lo que es recomendable revisar la documentación oficial de `pingouin` para obtener información detallada y ejemplos adicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset A, which is a Series of integers\n",
    "A = pd.read_csv(\"data/A.csv\")\n",
    "\n",
    "# Here, A[\"A\"] is the Series, our sample (its mean is 39.6, and the sample size is 244)\n",
    "# And 56 is the value to compare with\n",
    "pingouin.ttest(A[\"A\"], 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset B, which is another Series of integers\n",
    "B = pd.read_csv(\"data/B.csv\")\n",
    "\n",
    "# Here, B[\"B\"] is the second Series (its mean is 37.15, and the sample size is 215)\n",
    "pingouin.ttest(A[\"A\"],  B[\"B\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Anova.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Clase 2:  NPL - Natural Language Processing\n",
    "\n",
    "- Las librerias usadas para el procesamiento de lenguage son: **Spicy nltk y gensim**\n",
    "- Librerias para el procesamiento de texto son: **re, from string import formater, from string import template**\n",
    "\n",
    "A continuacion un ejemplo de cada una de las concatenaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello John!\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo base de concatenacion de strings\n",
    "\n",
    "name = \"John\"\n",
    "print('Hello '+ name +'!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello John!\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo base de formateo\n",
    "\n",
    "name = \"John\"\n",
    "print(\"Hello {}!\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello John!\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de formateo implicito con variables\n",
    "\n",
    "name = \"John\"\n",
    "print(f'Hello {name}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello John!\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de formateo usando el objeto Formatter de la libreria string\n",
    "\n",
    "from string import Formatter\n",
    "\n",
    "formatter = Formatter()\n",
    "\n",
    "print(formatter.format(\"Hello {name}!\", name = \"John\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello John!\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de formateo usando Template\n",
    "\n",
    "from string import Template\n",
    "\n",
    "t = Template(\"Hello $name!\")\n",
    "s = t.substitute(name = \"John\")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otras funciones de strings.\n",
    "\n",
    "- A continuacion un ejemplo de capitalizacion de texto y su diferencia con la funcion capitalize()\n",
    "- Luego un ejemplo de encoding, para ver el código de caracteres raros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you?\n"
     ]
    }
   ],
   "source": [
    "# Observe que solo el primer caracter de toda la frase empieza en mayúscula\n",
    "\n",
    "print('hello how are you?'.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello How Are You?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe que todas las palabras de la frase empiezan en mayúscula\n",
    "\n",
    "import string\n",
    "\n",
    "string.capwords(\"hello how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El string es:  pythön!\n",
      "Encode Version: b'pyth\\xc3\\xb6n!'\n"
     ]
    }
   ],
   "source": [
    "# Devuelve un objeto de tipo bytes, donde está la codificación de cada caracter raro.\n",
    "\n",
    "pytx_string = 'pythön!'\n",
    "print('El string es: ', pytx_string)\n",
    "str_utf = pytx_string.encode()\n",
    "print('Encode Version:', str_utf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expresiones Regulares\n",
    "\n",
    "Expresiones regulares es un sublenguaje que sirve para filtrar textos basado en patrones. este sublenguaje es transversal a todos los lenguajes de programacion, por lo que aprender esto es crucial para ser eficiente en el tratamiento de cadenas de texto.\n",
    "\n",
    "La mejor manera de aprender esto es practicando y entendiendo, estas 3 páginas son buenas para esto:\n",
    "\n",
    "- <a href='https://regexcrossword.com/'> regexcrossword</a>\n",
    "- <a href='https://regex101.com/'> regex 101</a>\n",
    "- <a href='https://regexr.com/'> regexr</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Busqueda sin expresiones regulares.\n",
    "\n",
    "frase = 'this is a sample string'\n",
    "\n",
    "print('is' in frase)\n",
    "print('xyz' in frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(2, 4), match='is'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Busqueda con expresiones regulares.\n",
    "\n",
    "import re\n",
    "\n",
    "re.search(r'is', frase)\n",
    "re.search(r'xyz', frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# La funcion match encuentra si la palabra se encuentra al inicio de la frase.\n",
    "\n",
    "import re\n",
    "\n",
    "print(bool(re.match(r'this', frase)))\n",
    "print(bool(re.match(r'is', frase)))\n",
    "print(bool(re.match(r'xyz', frase)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attempt', 'tattle']\n"
     ]
    }
   ],
   "source": [
    "# Busqueda con expresiones regulares dentro de una lista.\n",
    "\n",
    "palabras = ['cat', 'attempt', 'python', 'tattle']\n",
    "print([w for w in palabras if re.search(r'tt', w)])\n",
    "\n",
    "# Lo que se imprime dentro del print se le conoce como generador, otros lo llaman list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "# Esta es la prueba de que el tipo de dato es un generador.\n",
    "print(type(w for w in palabras if re.search(r'tt', w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All y Any\n",
    "\n",
    "En el código que has compartido, all y any son **funciones incorporadas en Python que se utilizan para evaluar condiciones booleanas en una secuencia de elementos**.\n",
    "\n",
    "**all(iterable)**: Esta función devuelve True si todos los elementos en el iterable son evaluados como verdaderos, y False si al menos uno de los elementos es evaluado como falso. En el código, la expresión all(re.search(r'tt', w) for w in palabras) verifica si la cadena 'tt' está presente en todas las palabras de la secuencia palabras.\n",
    "\n",
    "**any(iterable)**: Esta función devuelve True si al menos uno de los elementos en el iterable es evaluado como verdadero, y False si todos los elementos son evaluados como falsos. En el código, las expresiones any(re.search(r'stat', w) for w in palabras) y any(re.search(r'mp', w) for w in palabras) verifican si la cadena 'stat' o 'mp' está presente en al menos una de las palabras de la secuencia palabras.\n",
    "\n",
    "En resumen, all devuelve True si una condición es verdadera para todos los elementos de una secuencia, mientras que any devuelve True si una condición es verdadera para al menos uno de los elementos de una secuencia. Ambas funciones son útiles para realizar evaluaciones condicionales en una secuencia de elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(all(re.search(r'tt', w) for w in palabras))\n",
    "print(any(re.search(r'stat', w) for w in palabras))\n",
    "print(any(re.search(r'mp', w) for w in palabras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 're.Pattern'>\n",
      "<re.Match object; span=(14, 17), match='dog'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compilando la expresión regular\n",
    "pet = re.compile(r'dog')\n",
    "print(type(pet))\n",
    "# pet = re.compile(r'dog') es igual que re.search(r'dog')\n",
    "print(pet.search('They bought a dog'))\n",
    "print(pet.search('A cat crossed their path'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_data = b'this es a sample string'\n",
    "\n",
    "try:\n",
    "    re.search(r'is', byte_data)\n",
    "except TypeError:\n",
    "    print('TypeError')\n",
    "    print('No es posible usar expresiones regulares con bytes')\n",
    "    print(bool(re.search(rb'is', byte_data)))\n",
    "    print(bool(re.search(rb'xyz', byte_data)))\n",
    "finally:\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PBI_GDP_Country\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"requirements.txt\"\n",
    "\n",
    "word = re.compile(r'PBI')\n",
    "\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        if word.search(line):\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors\n",
    "Los \"anchors\" en expresiones regulares son caracteres especiales que se utilizan para indicar posiciones específicas en una cadena de texto. Estos anchors no coinciden con ningún carácter en sí, pero se utilizan para definir patrones que se aplican a una posición en particular dentro de la cadena.\n",
    "\n",
    "Aquí hay dos anchors comunes en expresiones regulares:\n",
    "\n",
    "**^ (circunflejo)**: Este anchor se utiliza para indicar el comienzo de una línea. Si se coloca al principio de una expresión regular, coincide con la posición de inicio de una cadena o la posición inmediatamente después de un salto de línea.\n",
    "\n",
    "**$ (signo de dólar)**: Este anchor se utiliza para indicar el final de una línea. Si se coloca al final de una expresión regular, coincide con la posición de finalización de una cadena o la posición inmediatamente antes de un salto de línea.\n",
    "\n",
    "Algunos ejemplos de uso de estos anchors en expresiones regulares son:\n",
    "```re\n",
    "^Hola: Coincide con la palabra \"Hola\" al comienzo de una línea.\n",
    "[0-9]$: Coincide con un dígito al final de una línea.\n",
    "^$: Coincide con una línea vacía.\n",
    "```\n",
    "Los anchors son útiles cuando se necesita especificar patrones que se ajusten solo a ciertas posiciones en una cadena de texto. Al utilizar estos anchors, se puede controlar con precisión dónde se deben encontrar los patrones en una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='cat'>\n",
      "None\n",
      "<re.Match object; span=(0, 2), match='hi'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.search(r'\\Acat', 'cater'))\n",
    "print(re.search(r'\\Acat', 'concatenation'))\n",
    "print(re.search(r'\\Ahi', 'hi hello \\n top spot'))\n",
    "print(re.search(r'\\Atop', 'hi hello \\n top spot'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(9, 13), match='tion'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'cat\\Z', 'cater')\n",
    "re.search(r'tion\\Z', 'concatenation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cater end'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\A', 'start: ', 'cater')\n",
    "re.sub(r'\\Z', ' end', 'cater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='cat'>\n",
      "<re.Match object; span=(8, 11), match='dog'>\n"
     ]
    }
   ],
   "source": [
    "pets = 'cat and dog'\n",
    "print(re.search(r'^cat', pets))\n",
    "print(re.search(r'dog$', pets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAR spar apparent spare PARt\n",
      "------ hello ------\n",
      "\"\"par\"\",\"\"spar\"\",\"\"apparent\"\",\"\"spare\"\",\"\"part\"\"\n",
      " foo_baz = num1 + 35 * 42 / num2 \n",
      "foo_baz = num1 + 35 * 42 / num2\n"
     ]
    }
   ],
   "source": [
    "words = 'par spar apparent spare part'\n",
    "\n",
    "print(re.sub(r'\\bpar', 'PAR', words))\n",
    "print(re.sub(r'\\b', ' ', '------hello------'))\n",
    "print(re.sub(r'\\b', '\"\"', words.replace(' ', ',')))\n",
    "print(re.sub(r'\\b', ' ', 'foo_baz=num1+35*42/num2'))\n",
    "print(re.sub(r'\\b', ' ', 'foo_baz=num1+35*42/num2').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "par sX apXent sXe part\n",
      " - - - - - -h e l l o- - - - - - \n",
      "pXaXr,sXpXaXr,aXpXpXaXrXeXnXt,sXpXaXrXe,pXaXrXt\n",
      "f o o _ b a z=n u m 1+3 5*4 2/n u m 2\n",
      "f o o _ b a z=n u m 1+3 5*4 2/n u m 2\n"
     ]
    }
   ],
   "source": [
    "words = 'par spar apparent spare part'\n",
    "\n",
    "print(re.sub(r'\\Bpar', 'X', words))\n",
    "print(re.sub(r'\\B', ' ', '------hello------'))\n",
    "print(re.sub(r'\\B', 'X', words.replace(' ', ',')))\n",
    "print(re.sub(r'\\B', ' ', 'foo_baz=num1+35*42/num2'))\n",
    "print(re.sub(r'\\B', ' ', 'foo_baz=num1+35*42/num2').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog']\n",
      "<re.Match object; span=(7, 10), match='dog'>\n",
      "I like mamifers and mamifers\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'cat|dog', 'I like cats and dogs'))\n",
    "print(re.search(r'cat|dog', 'I like dogs'))\n",
    "\n",
    "print(re.sub(r'cat|dog', 'mamifer', 'I like cats and dogs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unicodedata\n",
    "\n",
    "La biblioteca **unicodedata** en Python proporciona una interfaz para acceder a la base de datos Unicode y realizar operaciones relacionadas con caracteres Unicode. Esta biblioteca es útil para trabajar con cadenas de texto que contienen caracteres Unicode y realizar tareas como normalización, búsqueda y clasificación de caracteres.\n",
    "\n",
    "Algunas de las funcionalidades principales de la biblioteca unicodedata son las siguientes:\n",
    "\n",
    "1. **Normalización de texto**: La biblioteca unicodedata proporciona funciones para normalizar texto Unicode utilizando los estándares de normalización Unicode, como NFC (Forma Normalizada de Composición) y NFD (Forma Normalizada de Descomposición). Estas funciones permiten convertir caracteres Unicode en su forma canonizada y facilitan la comparación y manipulación de cadenas de texto Unicode.\n",
    "\n",
    "2. **Propiedades de caracteres**: Puedes utilizar la biblioteca unicodedata para obtener información sobre las propiedades de un carácter Unicode específico, como su categoría de carácter, nombre, código numérico y caso (mayúsculas o minúsculas). Esto es útil para realizar operaciones de clasificación y búsqueda de caracteres Unicode.\n",
    "\n",
    "3. **Conversión de caracteres**: La biblioteca unicodedata proporciona funciones para convertir caracteres Unicode en otras representaciones, como mayúsculas a minúsculas y viceversa, y conversión de caracteres de ancho completo a ancho medio.\n",
    "\n",
    "4. **Comparación de caracteres**: Puedes utilizar la biblioteca unicodedata para comparar caracteres Unicode y realizar operaciones de ordenación basadas en las propiedades de los caracteres.\n",
    "\n",
    "En resumen, la biblioteca **unicodedata** en Python brinda una interfaz para acceder a la base de datos Unicode y realizar tareas relacionadas con caracteres Unicode, como normalización, búsqueda, clasificación y conversión. Esta biblioteca es útil cuando se trabaja con cadenas de texto que contienen caracteres Unicode y se necesita manipular o analizar estos caracteres de manera específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def formating_text(text):\n",
    "    text = normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode('utf-8')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El mundo arabe es un gran pais del oriente'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formating_text('El mundo árabe es un gran país del oriente')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK\n",
    "\n",
    "La biblioteca Natural Language Toolkit (NLTK) es una biblioteca de procesamiento de lenguaje natural (NLP) en Python que proporciona herramientas y recursos para trabajar con texto y datos en lenguaje natural. NLTK es una biblioteca muy popular y ampliamente utilizada en la comunidad de NLP debido a su amplia gama de funcionalidades. Algunas de las principales características y funcionalidades de NLTK son las siguientes:\n",
    "\n",
    "- Tokenización.\n",
    "- Procesamiento de texto.\n",
    "- Etiquetado de partes del discurso.\n",
    "- Análisis gramatical y sintáctico.\n",
    "- Modelado de lenguaje.\n",
    "- Clasificación de texto.\n",
    "- Recursos lingüísticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from nltk import sent_tokenize # tokeniza frases\n",
    "from nltk.tokenize import word_tokenize # tokeniza palabras\n",
    "from nltk.corpus import stopwords # raiz de la palabras\n",
    "from nltk.stem.porter import PorterStemmer # frases\n",
    "nltk.download('punkt') # puntuacion\n",
    "nltk.download('stopwords') # Diccionarios de raiz de palabras\n",
    "nltk.download('wordnet') # Funcion de la palabra dentro de una frase, si es verbo, sustantivo, adjetivo, etc \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/imdb_dataset.csv',nrows=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retira las etiquetas HTML de un texto\n",
    "\n",
    "def striphtml(data):\n",
    "    '''\n",
    "    Strip HTML from text\n",
    "    '''\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(data.review[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.review = data.review.apply(striphtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizacion por oraciones\n",
    "sentences = sent_tokenize(data.review[1])\n",
    "print(sentences)\n",
    "print(sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizacion por palabras\n",
    "words = word_tokenize(sentences[3])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pone en minuscula todas las palabras de la lista\n",
    "words = [w.lower() for w in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtra la lista de palabras que no sean alfabeticas y que no sean stopwords\n",
    "words = [w for w in words if not w in stop_words and w.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "\n",
    "La biblioteca Gensim es una biblioteca de procesamiento de lenguaje natural (NLP) en Python que se utiliza para realizar tareas de modelado de temas y procesamiento de texto. Gensim proporciona una variedad de algoritmos y herramientas para trabajar con texto y documentos, incluyendo:\n",
    "\n",
    "1. **Modelado de temas**: Gensim ofrece implementaciones de algoritmos populares de modelado de temas, como Latent Dirichlet Allocation (LDA) y Latent Semantic Analysis (LSA). Estos algoritmos permiten descubrir temas ocultos en una colección de documentos y asignar palabras a esos temas.\n",
    "\n",
    "2. **Word2Vec**: Gensim incluye implementaciones del algoritmo Word2Vec, que se utiliza para representar palabras como vectores numéricos densos. Estas representaciones vectoriales pueden capturar relaciones semánticas y similitudes entre palabras.\n",
    "\n",
    "3. **Procesamiento de texto**: Gensim proporciona herramientas para preprocesar y transformar texto, como tokenización, eliminación de stopwords y normalización de palabras. Estas herramientas ayudan a preparar el texto para su posterior análisis y modelado.\n",
    "\n",
    "4. **Similitud de documentos**: Gensim permite calcular la similitud entre documentos utilizando modelos de espacio vectorial. Esto puede ser útil para tareas como búsqueda de documentos relacionados o recomendación de contenido.\n",
    "\n",
    "5. **Indexación de documentos**: Gensim proporciona estructuras de datos y algoritmos eficientes para indexar grandes colecciones de documentos, lo que permite realizar búsquedas rápidas y eficientes en el corpus de texto.\n",
    "\n",
    "En resumen, Gensim es una biblioteca de procesamiento de lenguaje natural en Python que ofrece herramientas y algoritmos para el modelado de temas, procesamiento de texto y análisis de documentos. Es ampliamente utilizado en aplicaciones de NLP, como la clasificación de texto, la agrupación de documentos y la extracción de temas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary.token2id)\n",
    "#> Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy \n",
    "\n",
    "spaCy es la libreria mas rapida de NPL hoy dia, Este está diseñado para usarse en aplicaciones reales y extraer informacion relevante. spaCy tambien es util para preparar texto para otras tareas de aprendizaje de maquina. Esto tambien le permte construir modelo sofisticados estadisticos linguisticos  para cualquier procesador de lenguaje natural para resolver problemas. Para mas, ver la documentacion.\n",
    "\n",
    "Para instalar spaCy corra los siguientes comandos desde la terminal (Por favor sea paciente, esto toma tiempo)\n",
    "```bash\n",
    "pip install spacy --user\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "Entonces usted puede copiar y correr el código que sige debajo para probarlo.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "file_name = 'data/programming_quotes.txt'\n",
    "introduction_file_text = open(file_name).read()\n",
    "introduction_file_doc = nlp(introduction_file_text)\n",
    "\n",
    "# Extract tokens for the given doc\n",
    "print ([token.text for token in introduction_file_doc])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separo frases por puntos\n",
    "# Esto es tokenizacion por sentencias\n",
    "\n",
    "# sentence tokenization\n",
    "sentences = nltk.sent_tokenize(AllReviews[0])\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words tokenization\n",
    "sentences = nltk.sent_tokenize(data['text'][1])\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(sentence)\n",
    "    print(words)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
