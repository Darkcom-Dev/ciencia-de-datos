{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation}\n",
    "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))]\n",
    "\\end{equation}\n",
    "\n",
    "- G es el generador.\n",
    "- D es el discriminador.\n",
    "- $p_{\\text{data}}(x)$ es la distribución real de los datos.\n",
    "- $p_z(z)$ es la distribución del ruido de entrada al generador.\n",
    "- $x$ es un dato real.\n",
    "- $z$ es un vector de ruido de entrada al generador.\n",
    "\n",
    "\n",
    "La función objetivo $V(D, G)$ es maximizada respecto a $D$ y minimizada respecto a $G$. El término $\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)]$  Representa la probabilidad de que el discriminador clasifique correctamente los datos reales como reales, mientras que  $\\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))]$ representa la probabilidad de que el discriminador clasifique incorrectamente los datos generados como reales.\n",
    "\n",
    "Ahora hablemos del fenómeno de **saturación**. En algunos casos, las GANs pueden sufrir de saturación, donde el generador produce muestras que son muy similares entre sí y no reflejan la diversidad de los datos reales. Esto puede ocurrir cuando el discriminador se vuelve demasiado efectivo y clasifica todas las muestras generadas como falsas. Como resultado, el gradiente que recibe el generador puede volverse muy pequeño y no actualizar efectivamente los parámetros del generador.\n",
    "\n",
    "Una forma de abordar este problema es utilizar técnicas de regularización como el suavizado de etiquetas en el discriminador o el uso de funciones de pérdida más complejas que fomenten la diversidad en las muestras generadas. Además, el uso de arquitecturas de redes neuronales más avanzadas y técnicas de entrenamiento como el aprendizaje semi-supervisado también puede ayudar a mitigar el problema de la saturación.\n",
    "\n",
    "En resumen, las GANs son modelos poderosos para la generación de datos realistas, pero pueden enfrentar desafíos como el fenómeno de saturación. Sin embargo, con técnicas adecuadas de entrenamiento y regularización, es posible mejorar el rendimiento y la diversidad de las muestras generadas por las GANs.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentación de jupyter\n",
    "\n",
    "https://jupyter-notebook.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curso coolab\n",
    "\n",
    "https://colab.research.google.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "[Pepe Cantoral](https://www.youtube.com/@PepeCantoralPhD/playlists)\n",
    "[Alternativas de uso de GPU en la red](https://www.youtube.com/watch?v=CkYmjKUX-Sc)\n",
    "\n",
    "Links mencionados durante el video:\n",
    "- [Paperspace Gradient](https://github.com/huggingface/diffusers):  \n",
    "- [Introducción a Amazon SageMaker](https://www.youtube.com/watch?v=FUEIwAsrMP4).\n",
    "- [Kaggle Notebooks](https://www.kaggle.com/): \n",
    "- [Vast.ai](https://vast.ai/): \n",
    "- [FluidStack](https://www.fluidstack.io/pricing): \n",
    "- [RunPod](https://www.runpod.io):  \n",
    "\n",
    "- [Aprendizaje no supervisado, recomendadores, aprendizaje por refuerzo](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning)\n",
    "- [Embeding tensorflow](https://projector.tensorflow.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
